{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import write\n",
    "from python_speech_features import mfcc\n",
    "from python_speech_features.base import delta\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(389, 13)\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "mfcc_features = []\n",
    "delta_features = []\n",
    "english_features = []\n",
    "num_iterations = 0\n",
    "for file in os.listdir(\"C:\\\\Users\\Lenovo\\Desktop\\\\Utility\\Semester 5\\Machine Learning\\Project\\English_Data\\\\\"):\n",
    "    data, samplerate = sf.read(\"C:\\\\Users\\Lenovo\\Desktop\\\\Utility\\Semester 5\\Machine Learning\\Project\\English_Data\\\\\" + file)\n",
    "    hamming_window = numpy.hamming(400)\n",
    "    temp_mfcc_features = []\n",
    "    for i in range(0,len(data)-400,240):\n",
    "        trimmed = data[i:i+400]\n",
    "        hammed = numpy.multiply(hamming_window, trimmed)\n",
    "        mfcced = mfcc(hammed,samplerate)\n",
    "        mfcc_features.append(mfcced[0])\n",
    "        temp_mfcc_features.append(mfcced[0])\n",
    "        num_iterations+=1\n",
    "#     hamming_window = numpy.hamming(len(data)-(i+240))\n",
    "#     trimmed = data[i+240:len(data)]\n",
    "#     hammed = (numpy.multiply(hamming_window, trimmed))\n",
    "#     mfcced = mfcc(hammed,samplerate)\n",
    "#     mfcc_features.append(mfcced[0])\n",
    "#     temp_mfcc_features.append(mfcced[0])\n",
    "#     mfcc_features = numpy.array(mfcc_features)\n",
    "    print(delta(temp_mfcc_features, 1).shape)\n",
    "    delta_features.extend(delta(temp_mfcc_features, 1))\n",
    "    break\n",
    "    if (num_iterations>=5000):\n",
    "        break\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ... -6.10351562e-05\n",
      " -6.10351562e-05 -9.15527344e-05]\n",
      "[ 0.          0.          0.         ... -0.00918579  0.02752686\n",
      "  0.03726196]\n",
      "[ 0.          0.          0.         ...  0.00012207 -0.00164795\n",
      " -0.0012207 ]\n"
     ]
    }
   ],
   "source": [
    "hindi_mfcc_features = []\n",
    "hindi_delta_features = []\n",
    "hindi_features = []\n",
    "num_iterations = 0\n",
    "for file in os.listdir(\"C:\\\\Users\\Lenovo\\Desktop\\\\Utility\\Semester 5\\Machine Learning\\Project\\Hindi_Data_flac\\\\\"):\n",
    "    data, samplerate = sf.read(\"C:\\\\Users\\Lenovo\\Desktop\\\\Utility\\Semester 5\\Machine Learning\\Project\\Hindi_Data_flac\\\\\" + file)\n",
    "    hamming_window = numpy.hamming(400)\n",
    "    data = data[:,0]\n",
    "    temp_mfcc_features = []\n",
    "    print (data)\n",
    "    for i in range(0,len(data)-400,240):\n",
    "        trimmed = data[i:i+400]\n",
    "        hammed = numpy.multiply(hamming_window, trimmed)\n",
    "        mfcced = mfcc(hammed,samplerate,nfft=2048)\n",
    "        hindi_mfcc_features.append(mfcced[0])\n",
    "        temp_mfcc_features.append(mfcced[0])\n",
    "        num_iterations+=1\n",
    "#         break\n",
    "#     break\n",
    "#     hamming_window = numpy.hamming(len(data)-(i+240))\n",
    "#     trimmed = data[i+240:len(data)]\n",
    "#     hammed = (numpy.multiply(hamming_window, trimmed))\n",
    "#     mfcced = mfcc(hammed,samplerate)\n",
    "#     hindi_mfcc_features.append(mfcced[0])\n",
    "#     temp_mfcc_features.append(mfcced[0])\n",
    "#     mfcc_features = numpy.array(mfcc_features)\n",
    "    hindi_delta_features.extend(delta(temp_mfcc_features, 1))\n",
    "    if (num_iterations>=5000):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5132 5132\n"
     ]
    }
   ],
   "source": [
    "print(len(mfcc_features), len(delta_features))\n",
    "if (len(hindi_mfcc_features) != len(hindi_delta_features)):\n",
    "    print(\"PROBLEM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = [0]*len(mfcc_features)\n",
    "mfcc_features = numpy.array(mfcc_features)\n",
    "delta_features = numpy.array(delta_features)\n",
    "x_English = numpy.concatenate((mfcc_features,delta_features),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = [1]*len(hindi_mfcc_features)\n",
    "hindi_mfcc_features = numpy.array(hindi_mfcc_features)\n",
    "hindi_delta_features = numpy.array(hindi_delta_features)\n",
    "x_Hindi = numpy.concatenate((hindi_mfcc_features, hindi_delta_features),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1018, 1: 1109} hi\n",
      "1109 2127\n",
      "0.9717912552891397\n"
     ]
    }
   ],
   "source": [
    "X = numpy.concatenate((x_English, x_Hindi), axis=0)\n",
    "Y = numpy.concatenate((zeros, ones), axis=0)\n",
    "normalized_X = preprocessing.normalize(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(normalized_X, Y, test_size = 0.2)\n",
    "# parameters = [{'kernel': ['rbf'], 'gamma' : [0.001,0.05,0.1,10], 'C' : [0.001, 0.01, 0.1, 1,2.5,100]}]\n",
    "# clf = GridSearchCV(svm.SVC(), parameters, cv = 5)\n",
    "# clf = GridSearchCV(svm.SVC())\n",
    "clf = svm.SVC()\n",
    "clf.fit(x_train, y_train)\n",
    "predictions = clf.predict(x_test)\n",
    "unique, counts = numpy.unique(predictions, return_counts=True)\n",
    "print(dict(zip(unique, counts)), \"hi\")\n",
    "# print(predictions.count(1))\n",
    "print(sum(predictions),len(predictions))\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6478848, 2)\n"
     ]
    }
   ],
   "source": [
    "mfcc_features = []\n",
    "delta_features = []\n",
    "test_English = []\n",
    "data, samplerate = sf.read(\"C:\\\\Users\\Lenovo\\Desktop\\\\Utility\\Semester 5\\Machine Learning\\Project\\Sample_English.flac\")\n",
    "print(data.shape)\n",
    "data = data[:,0]\n",
    "num_iterations = 0\n",
    "hamming_window = numpy.hamming(400)\n",
    "temp_mfcc_features = []\n",
    "for i in range(0,len(data)-400,240):\n",
    "    trimmed = data[i:i+400]\n",
    "    hammed = numpy.multiply(hamming_window, trimmed)\n",
    "    mfcced = mfcc(hammed, samplerate, nfft = 2048)\n",
    "    mfcc_features.append(mfcced[0])\n",
    "    temp_mfcc_features.append(mfcced[0])\n",
    "    num_iterations+=1\n",
    "    if (num_iterations>=1000):\n",
    "        break\n",
    "delta_features.extend(delta(temp_mfcc_features, 1))\n",
    "\n",
    "\n",
    "test_English = numpy.concatenate((mfcc_features,delta_features),axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 0 0 0 1 0 0 1 1 1 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 0 0 0\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 1\n",
      " 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 0 0 1 0 0 1 0 0 1 0 0 0 0 1\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 0 1 0 0 1 1 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 0 1 0 0 1 1 0 0 1 1\n",
      " 1 0 1 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0\n",
      " 1 0 0 1 0 0 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1 0\n",
      " 1 1 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 1 1 0\n",
      " 0]\n",
      "664\n",
      "0.33599999999999997\n"
     ]
    }
   ],
   "source": [
    "test_English = preprocessing.normalize(test_English)\n",
    "results = clf.predict(test_English)\n",
    "print(results)\n",
    "print(sum(results))\n",
    "print (1-sum(results)/len(test_English))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5870592, 2)\n"
     ]
    }
   ],
   "source": [
    "mfcc_features = []\n",
    "delta_features = []\n",
    "test_English = []\n",
    "data, samplerate = sf.read(\"C:\\\\Users\\Lenovo\\Desktop\\\\Utility\\Semester 5\\Machine Learning\\Project\\Sample_Hindi.flac\")\n",
    "print(data.shape)\n",
    "data = data[:,0]\n",
    "num_iterations = 0\n",
    "hamming_window = numpy.hamming(400)\n",
    "temp_mfcc_features = []\n",
    "for i in range(0,len(data)-400,240):\n",
    "    trimmed = data[i:i+400]\n",
    "    hammed = numpy.multiply(hamming_window, trimmed)\n",
    "    mfcced = mfcc(hammed, samplerate, nfft = 2048)\n",
    "    mfcc_features.append(mfcced[0])\n",
    "    temp_mfcc_features.append(mfcced[0])\n",
    "    num_iterations+=1\n",
    "    if (num_iterations>=1000):\n",
    "        break\n",
    "delta_features.extend(delta(temp_mfcc_features, 1))\n",
    "\n",
    "\n",
    "test_English = numpy.concatenate((mfcc_features,delta_features),axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_English = preprocessing.normalize(test_English)\n",
    "results = clf.predict(test_English)\n",
    "print(results)\n",
    "print(sum(results))\n",
    "print (sum(results)/len(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
